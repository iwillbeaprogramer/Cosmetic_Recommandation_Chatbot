{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Created on 2018. 2. 14.\n",
    "\n",
    "@author: phs\n",
    "\n",
    "1. 대화 말뭉치 파일을 읽어들인다.\n",
    "2. 딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "3. 딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "4. 입력받은 문장을 자연어 처리한다(토크나이저, 스태밍).\n",
    "5. 자연어 처리한 문장을 Bag of word 생성하여 반환한다.\n",
    "6. 입력받은 문장을 학습 모델(자연어 이해 모델)을 이용 분류하여  결과를 반환한다.\n",
    "7. 분류된 말뭉치 대화에서 임의로 한 문장을 선택하여 입력받은 문장의 대답으로 반혼한다.\n",
    "\n",
    "'''\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "# things we need for NLP\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "# komoran = Komoran()\n",
    "twitter = Okt()\n",
    "\n",
    "\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "def read_dialog_intents_jsonfile(input_file_name):\n",
    "    \"\"\"\n",
    "     대화 말뭉치 파일을 읽어들인다.\n",
    "    \"\"\"\n",
    "    with open(input_file_name, 'rt', encoding='UTF8') as json_data:\n",
    "        intents = json.load(json_data)\n",
    "        \n",
    "    return intents\n",
    "\n",
    "# 대화 말뭉치와 대화 의도가 정의된 JSON 문서 집합 읽기\n",
    "input_file_name = './DialogIntents/intents_home_kr.json'\n",
    "intents = read_dialog_intents_jsonfile(input_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "def restore_training_data_structures(input_training_data_file_name):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어 들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # restore all of our data structures\n",
    "    data = pickle.load( open( input_training_data_file_name, \"rb\" ) )\n",
    "    words = data['words']\n",
    "    classes = data['classes']\n",
    "    train_x = data['train_x']\n",
    "    train_y = data['train_y']\n",
    "    \n",
    "    return classes, words, train_x, train_y\n",
    "\n",
    "#  딥러닝(tensorflow) 모델 생성에 사용한 자료구조를 읽어들임\n",
    "input_training_data_file_name = \"./NLUModel/training_data_home_kr\"\n",
    "classes, words, train_x, train_y = restore_training_data_structures(input_training_data_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# restore all of trained tflearn model file\n",
    "def restore_training_model(train_x, train_y, tflearn_logs_dir):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # Build neural network\n",
    "    net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, 8)\n",
    "    net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "    net = tflearn.regression(net)\n",
    "    \n",
    "    # Define model and setup tensorboard\n",
    "    model = tflearn.DNN(net, tensorboard_dir=tflearn_logs_dir)\n",
    "\n",
    "    return model\n",
    "\n",
    "#  딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들임\n",
    "tflearn_logs_dir = \"home_tflearn_kr_logs\"\n",
    "model = restore_training_model(train_x, train_y, tflearn_logs_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restore all of trained tflearn model file\n",
    "def restore_training_model(train_x, train_y, tflearn_logs_dir):\n",
    "    \"\"\"\n",
    "     딥러닝(tensorflow)으로 생성한 자연어 이해 모델을 읽어들여서 반환한다.\n",
    "    \"\"\"\n",
    "    # Build neural network\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(8, input_shape=(len(train_x[0]))),\n",
    "        tf.keras.layers.Dense(8),\n",
    "        tf.keras.layers.Dense(len(train_y[0]), activation=\"softmax\"),\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    model.fit(train_x, train_y, epochs=1000, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\saint\\Anaconda3\\envs\\chat_env\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\saint\\GroupProject\\step4\\jupyter_app\\nlp\\home_chat\\NLUModel\\model_home_kr.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "tflearn_model_file_name = \"./NLUModel/model_home_kr.tflearn\"\n",
    "model.load(tflearn_model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# things we need for Tensorflow\n",
    "\n",
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "#     print(\"sentence pos_result[%s]\" % pos_result)\n",
    "#     print(\"sentence pos_result[0][0][%s]\" % pos_result[0][0])\n",
    "#     print(\"sentence pos_result[1][0][%s]\" % pos_result[1][0])\n",
    "#     print(\"sentence pos_result[][%s]\" % pos_result[0])\n",
    "    sentence_words = [lex for lex, pos in pos_result]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "\n",
    "def classify(sentence, words, classes, model):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, intents, words, classes, model, userID='123', show_details=False):\n",
    "    results = classify(sentence, words, classes, model)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # set context for this intent if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', i['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return random.choice(i['responses'])\n",
    "\n",
    "            results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('안녕?')[[('greeting', 0.99066764)]]\n",
      "response('안녕?') ==> [다시 만나서 반가워요.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '안녕?'\n",
    "print(\"classify('안녕?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('안녕?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('무슨 제품이 있나요?')[[('product', 0.999616)]]\n",
      "response('무슨 제품이 있나요?') ==> [채팅 대화하는 홈페이지, 채팅 대화 고객 FAQ, 채팅 대화 고객 문의 자동 응답 시스템을 클라우드 서비스와 패키지 기반 구축 서비스로 제공합니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '무슨 제품이 있나요?'\n",
    "print(\"classify('무슨 제품이 있나요?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('무슨 제품이 있나요?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('구체적인 기술은?')[[('possessTechnology', 0.9998584)]]\n",
      "response('구체적인 기술은?') ==> [부트스트랩 (CSS 프레임워크),jQuery,머신러닝, TensorFlow,자연어처리,자연어이해,Python 등 입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '구체적인 기술은?'\n",
    "print(\"classify('구체적인 기술은?')[%s]\" % classify(sentence, words, classes, model))\n",
    "print(\"response('구체적인 기술은?') ==> [%s]\" % response(sentence, intents, words, classes, model))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': 'serviceTechnology'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('제품기술')[[('possessTechnology', 0.95949155)]]\n",
      "response('제품기술') ==> [부트스트랩 (CSS 프레임워크),jQuery,머신러닝, TensorFlow,자연어처리,자연어이해,Python 등 입니다.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '제품기술'\n",
    "print(\"classify('제품기술')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('제품기술') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n",
      "response('안녕?') ==> [안녕하세요, 어떻게 도와 드릴까요?]\n",
      "classify('안녕?')[[('greeting', 0.9875065)]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clear context\n",
    "#response(\"Hi there!\", show_details=True)\n",
    "sentence = '안녕'\n",
    "print(\"response('안녕?') ==> [{}]\".format(response(sentence, intents, words, classes, model,show_details=True)))\n",
    "print(\"classify('안녕?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('서비스기술')[[('possessTechnology', 0.90978044)]]\n",
      "response('서비스기술') ==> [None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '서비스기술'\n",
    "print(\"classify('서비스기술')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('서비스기술') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('안녕히 계셔요.')[[('goodbye', 0.9815683)]]\n",
      "response('안녕히 계셔요.') ==> [안녕히 가세요]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '안녕히 계셔요.'\n",
    "print(\"classify('안녕히 계셔요.')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('안녕히 계셔요.') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'123': ''}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show context\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify('뭐야?')[[('greeting', 0.34372285), ('Slang', 0.3057799)]]\n",
      "response('뭐야?') ==> [다시 만나서 반가워요.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = '뭐야?'\n",
    "print(\"classify('뭐야?')[{}]\".format(classify(sentence, words, classes, model)))\n",
    "print(\"response('뭐야?') ==> [{}]\".format(response(sentence, intents, words, classes, model)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChatApp_env",
   "language": "python",
   "name": "chat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
