{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I compare document similarity using Python?\n",
    "Learn how to use the gensim Python library to determine the similarity between two or more documents.\n",
    "\n",
    "By Jonathan Mugan April 18, 2017\n",
    "\n",
    "https://www.oreilly.com/learning/how-do-i-compare-document-similarity-using-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How do I find documents similar to a particular document?\n",
    "\n",
    "We will use a library in Python called gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phs\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NullHandler', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'corpora', 'interfaces', 'logger', 'logging', 'matutils', 'models', 'parsing', 'scripts', 'similarities', 'summarization', 'topic_coherence', 'utils']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(dir(gensim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's create some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "             \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "             \"Legend has it that the mind is a mad monkey.\",\n",
    "            \"I make my own fun.\"]\n",
    "print(\"Number of documents:\",len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한글문장 갯수: 10\n"
     ]
    }
   ],
   "source": [
    "kor_documents = [\n",
    "        \"인공지능의 발전과 고용의 미래\",\n",
    "        \"인공지능, 로봇, 빅데이터와 제4차 산업혁명\",\n",
    "        \"인공지능 시대의 법적·윤리적 쟁점\",\n",
    "        \"인공지능(AI) 시대, 교회 공동체 성립요건 연구: 예배와 설교 가능성을 중심으로\",\n",
    "        \"게임 인공지능 연구동향\",\n",
    "        \"파고(인공지능)의 번역 불가능성과 디지털 바벨탑의 붕괴 - 언어의 이종성異種性이 불러올 미래에 대한 가상 시놉시스 -\",\n",
    "        \"인공지능과 심층학습의 발전사\",\n",
    "        \"일반 비디오 게임 플레이 인공지능을 위한 GreedyUCB1기반 몬테카를로 트리 탐색\",\n",
    "        \"포스트휴먼시대 인공지능과 미래 경제 트렌드\",\n",
    "        \"컴퓨터 게임에서의 인공지능 기술\"\n",
    "    ]     \n",
    "print(\"한글문장 갯수:\",len(kor_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will use NLTK to tokenize.\n",
    "\n",
    "A document will now be a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]\n",
    "print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['인공지능', '의', '발전', '과', '고용', '의', '미래'], ['인공지능', ',', '로봇', ',', '빅데이터', '와', '제4차 산업', '혁명'], ['인공지능', '시대', '의', '법', '적', '·', '윤리', '적', '쟁점'], ['인공지능', '(', 'AI', ')', '시대', ',', '교회', '공동체', '성립', '요건', '연구', ':', '예배', '와', '설교', '가능', '성', '을', '중심', '으로'], ['게임', '인공지능', '연구', '동향'], ['파고', '(', '인공지능', ')', '의', '번역', '불', '가능', '성', '과', '디지털', '바벨탑', '의', '붕괴', '-', '언어', '의', '이종성', '異種性', '이', '불러오', 'ㄹ', '미래', '에', '대하', 'ㄴ', '가상', '시놉시스', '-'], ['인공지능', '과', '심층', '학습', '의', '발전', '사'], ['일반', '비디오 게임', '플레이', '인공지능', '을', '위하', 'ㄴ', 'GreedyUCB', '1', '기반', '몬테카를로', '트리', '탐색'], ['포스트', '휴먼', '시대', '인공지능', '과', '미래', '경제', '트렌드'], ['컴퓨터 게임', '에서', '의', '인공지능', '기술']]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "gen_kor_docs = [[w for w in komoran.morphs(text)] \n",
    "            for text in kor_documents]\n",
    "print(gen_kor_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We will create a dictionary from a list of documents. \n",
    "A dictionary maps every word to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary[5])\n",
    "print(dictionary.token2id['road'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미래\n",
      "4\n",
      "한국어 Dictionary의 단어수: 74\n",
      "0 인공지능\n",
      "1 의\n",
      "2 발전\n",
      "3 과\n",
      "4 고용\n",
      "5 미래\n",
      "6 ,\n",
      "7 로봇\n",
      "8 빅데이터\n",
      "9 와\n",
      "10 제4차 산업\n",
      "11 혁명\n",
      "12 시대\n",
      "13 법\n",
      "14 적\n",
      "15 ·\n",
      "16 윤리\n",
      "17 쟁점\n",
      "18 (\n",
      "19 AI\n",
      "20 )\n",
      "21 교회\n",
      "22 공동체\n",
      "23 성립\n",
      "24 요건\n",
      "25 연구\n",
      "26 :\n",
      "27 예배\n",
      "28 설교\n",
      "29 가능\n",
      "30 성\n",
      "31 을\n",
      "32 중심\n",
      "33 으로\n",
      "34 게임\n",
      "35 동향\n",
      "36 파고\n",
      "37 번역\n",
      "38 불\n",
      "39 디지털\n",
      "40 바벨탑\n",
      "41 붕괴\n",
      "42 -\n",
      "43 언어\n",
      "44 이종성\n",
      "45 異種性\n",
      "46 이\n",
      "47 불러오\n",
      "48 ㄹ\n",
      "49 에\n",
      "50 대하\n",
      "51 ㄴ\n",
      "52 가상\n",
      "53 시놉시스\n",
      "54 심층\n",
      "55 학습\n",
      "56 사\n",
      "57 일반\n",
      "58 비디오 게임\n",
      "59 플레이\n",
      "60 위하\n",
      "61 GreedyUCB\n",
      "62 1\n",
      "63 기반\n",
      "64 몬테카를로\n",
      "65 트리\n",
      "66 탐색\n",
      "67 포스트\n",
      "68 휴먼\n",
      "69 경제\n",
      "70 트렌드\n",
      "71 컴퓨터 게임\n",
      "72 에서\n",
      "73 기술\n"
     ]
    }
   ],
   "source": [
    "kor_dictionary = gensim.corpora.Dictionary(gen_kor_docs)\n",
    "print(kor_dictionary[5])\n",
    "print(kor_dictionary.token2id['고용'])\n",
    "print(\"한국어 Dictionary의 단어수:\",len(kor_dictionary))\n",
    "for i in range(len(kor_dictionary)):\n",
    "    print(i, kor_dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we will create a corpus. \n",
    "A corpus is a list of bags of words. A bag-of-words representation for a document just lists the number of times each word occurs in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1)], [(0, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)], [(0, 1), (1, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1)], [(0, 1), (6, 1), (9, 1), (12, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(0, 1), (25, 1), (34, 1), (35, 1)], [(0, 1), (1, 3), (3, 1), (5, 1), (18, 1), (20, 1), (29, 1), (30, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1)], [(0, 1), (1, 1), (2, 1), (3, 1), (54, 1), (55, 1), (56, 1)], [(0, 1), (31, 1), (51, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1)], [(0, 1), (3, 1), (5, 1), (12, 1), (67, 1), (68, 1), (69, 1), (70, 1)], [(0, 1), (1, 1), (71, 1), (72, 1), (73, 1)]]\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = [kor_dictionary.doc2bow(gen_kor_doc) for gen_kor_doc in gen_kor_docs]\n",
    "print(kor_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we create a tf-idf model from the corpus. Note that num_nnz is the number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfModel(num_docs=10, num_nnz=104)\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "kor_tf_idf = gensim.models.TfidfModel(kor_corpus)\n",
    "print(kor_tf_idf)\n",
    "s = 0\n",
    "for i in kor_corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now we will create a similarity measure object in tf-idf space.\n",
    "\n",
    "tf-idf stands for term frequency-inverse document frequency. Term frequency is how often the word shows up in the document and inverse document fequency scales the value by how rare the word is in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('../note/workdir/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))\n",
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index with 10 documents in 0 shards (stored under ../note/ko_workdir/)\n",
      "<class 'gensim.similarities.docsim.Similarity'>\n"
     ]
    }
   ],
   "source": [
    "kor_sims = gensim.similarities.Similarity('../note/ko_workdir/',kor_tf_idf[kor_corpus],\n",
    "                                      num_features=len(kor_dictionary))\n",
    "print(kor_sims)\n",
    "print(type(kor_sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now create a query document and convert it to tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"Socks are a force for good.\")]\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['컴퓨터 게임', '에서', '의', '인공지능', '기술']\n",
      "[(0, 1), (1, 1), (71, 1), (72, 1), (73, 1)]\n",
      "[(1, 0.1712328295110301), (71, 0.5688231471197489), (72, 0.5688231471197489), (73, 0.5688231471197489)]\n"
     ]
    }
   ],
   "source": [
    "kor_query_doc = komoran.morphs(\"컴퓨터 게임에서의 인공지능 기술\")\n",
    "print(kor_query_doc)\n",
    "\n",
    "kor_query_doc_bow = kor_dictionary.doc2bow(kor_query_doc)\n",
    "print(kor_query_doc_bow)\n",
    "\n",
    "kor_query_doc_tf_idf = kor_tf_idf[kor_query_doc_bow]\n",
    "print(kor_query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We show an array of document similarities to query. We see that the second document is the most similar with the overlapping of socks and force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06823284,  0.        ,  0.01782335,  0.        ,  0.        ,\n",
       "        0.03177125,  0.02666271,  0.        ,  0.        ,  1.        ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sims[kor_query_doc_tf_idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0682328\n",
      "1 0.0\n",
      "2 0.0178233\n",
      "3 0.0\n",
      "4 0.0\n",
      "5 0.0317713\n",
      "6 0.0266627\n",
      "7 0.0\n",
      "8 0.0\n",
      "9 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(kor_sims[kor_query_doc_tf_idf])):\n",
    "    print(i, kor_sims[kor_query_doc_tf_idf][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
